{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJHu3NYuUJn9"
      },
      "source": [
        "\n",
        "## Lunar Lander V3 - Deep Q-Learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CnaDqSOUSDK"
      },
      "source": [
        "```\n",
        " pip install swig\n",
        " pip install gymnasium[box2d]\n",
        "```\n",
        "Imports necessários:\n",
        "\n",
        "\n",
        "*   OS\n",
        "*   Random\n",
        "*   Numpy\n",
        "*   Torch\n",
        "*   Gymnasium\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EtOtA-a5TT2q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn # P/ Rede Neural\n",
        "import torch.optim as optim # Otimizador\n",
        "import torch.nn.functional as F # Funções de ativação, perda, convolução, etc (ReLU)\n",
        "import torch.autograd as autograd # Diferenciação automática (gradiente)\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zowvpGAfTw8P",
        "outputId": "ff1bf229-37f0-4d3f-ad9b-e51641ed997e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State shape:  (8,)\n",
            "State size:  8\n",
            "Number of actions:  4\n",
            "Observation space:  Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n"
          ]
        }
      ],
      "source": [
        "# Biblioteca Gymnasium\n",
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode=\"human\")\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n # 0 - Do Nothing, 1 - Fire left, 2 - Fire down, 3 - Fire right\n",
        "observation_space = env.observation_space\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)\n",
        "print('Observation space: ', observation_space)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TaCPvqVJTyVH"
      },
      "outputs": [],
      "source": [
        "# Parâmetros de aprendizado\n",
        "learning_rate = 5e-4\n",
        "batch_size =100 #\n",
        "discount_factor = 0.99 # gamma\n",
        "replay_buffer_size = int(1e5)\n",
        "interpolation_parameter = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8iUtccNoXKEw"
      },
      "outputs": [],
      "source": [
        "# Rede Neural\n",
        "class Network(nn.Module):\n",
        "  def __init__(self,state_size,number_actions,seed=42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64) # 3 Camadas de neurônios totalmente conectadas\n",
        "    self.fc3 = nn.Linear(64, number_actions)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    q_values = self.fc3(x)\n",
        "    return q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xe4jl_Rn180Y"
      },
      "outputs": [],
      "source": [
        "# Memória de aprendizado\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, event):\n",
        "        self.memory.append(event)\n",
        "        # Deleta a memória mais antiga se estiver cheio (capacidade).\n",
        "        if len(self.memory) > self.capacity:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Amostragem aleatória do tamanho do batch size\n",
        "        experiences = random.sample(self.memory, k = batch_size)\n",
        "        # Converte em tensores pytorch para serem processados pela rede neural\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "        return (states, actions, rewards, next_states, dones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DthgH9iF_pbk"
      },
      "outputs": [],
      "source": [
        "class DQN():\n",
        "  def __init__(self,state_size,number_actions, batch_size=100):\n",
        "    self.batch_size = batch_size\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size =state_size\n",
        "    self.action_size = number_actions\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "    self.model = Network(state_size,number_actions).to(self.device)\n",
        "    self.target_model = Network(state_size,number_actions).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "    self.t_step=0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > self.batch_size:\n",
        "        experiences = self.memory.sample(self.batch_size)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.model(state)\n",
        "    self.model.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_q_targets = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.model(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.model, self.target_model, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "brain = DQN(state_size, number_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: -171.69\n",
            "Episode 200\tAverage Score: -131.96\n",
            "Episode 300\tAverage Score: -83.702\n",
            "Episode 400\tAverage Score: -14.09\n",
            "Episode 406\tAverage Score: -10.69"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maximum_number_timesteps_per_episode):\n\u001b[32m     13\u001b[39m   action = brain.act(state, epsilon)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m   next_state, reward, done, _, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m   brain.step(state, action, reward, next_state, done)\n\u001b[32m     16\u001b[39m   state = next_state\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:665\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    662\u001b[39m     reward = +\u001b[32m100\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(state, dtype=np.float32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:778\u001b[39m, in \u001b[36mLunarLander.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.screen.blit(\u001b[38;5;28mself\u001b[39m.surf, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    777\u001b[39m     pygame.event.pump()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrender_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     pygame.display.flip()\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = brain.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    brain.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(brain.model.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OAb07pc22YqG",
        "outputId": "abc12cac-a58c-4d13-e22f-74df25a7d751"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected bytes, NoneType found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     51\u001b[39m     imageio.mimsave(\u001b[33m'\u001b[39m\u001b[33mvideo.mp4\u001b[39m\u001b[33m'\u001b[39m, frames, fps=\u001b[32m30\u001b[39m) \u001b[38;5;66;03m# Salva os frames como um vídeo .mp4 com 30 quadros por segundo\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Chamada da função. Certifique-se que 'brain' é sua instância do agente DQN.\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Ex: brain = DQN(state_size, number_actions)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Se você tiver um modelo treinado salvo:\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# brain.load() \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mshow_video_of_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLunarLander-v3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_video\u001b[39m():\n\u001b[32m     60\u001b[39m     mp4list = glob.glob(\u001b[33m'\u001b[39m\u001b[33m*.mp4\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Procura arquivos .mp4 no diretório\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mshow_video_of_model\u001b[39m\u001b[34m(dqn, env_name)\u001b[39m\n\u001b[32m     45\u001b[39m               \u001b[38;5;66;03m# Se quiser vídeo mais longo, reinicie o ambiente e continue o loop:\u001b[39;00m\n\u001b[32m     46\u001b[39m               \u001b[38;5;66;03m# state, _ = env.reset()\u001b[39;00m\n\u001b[32m     47\u001b[39m               \u001b[38;5;66;03m# current_step = 0 # Reinicia o contador de passos para o novo episódio\u001b[39;00m\n\u001b[32m     48\u001b[39m               \u001b[38;5;66;03m# if len(frames) >= 3000: break # Adicione uma condição para parar de coletar frames totais\u001b[39;00m\n\u001b[32m     50\u001b[39m env.close() \u001b[38;5;66;03m# Fecha o ambiente após terminar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mimageio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvideo.mp4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\imageio\\v2.py:495\u001b[39m, in \u001b[36mmimwrite\u001b[39m\u001b[34m(uri, ims, format, **kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m imopen_args[\u001b[33m\"\u001b[39m\u001b[33mlegacy_mode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[33m\"\u001b[39m\u001b[33mwI\u001b[39m\u001b[33m\"\u001b[39m, **imopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\imageio\\plugins\\pyav.py:674\u001b[39m, in \u001b[36mPyAVPlugin.write\u001b[39m\u001b[34m(self, ndimage, codec, is_batch, fps, in_pixel_format, out_pixel_format, filter_sequence, filter_graph)\u001b[39m\n\u001b[32m    671\u001b[39m     ndimage = np.asarray(ndimage)\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._video_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_video_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_pixel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.set_video_filter(filter_sequence, filter_graph)\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m ndimage:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\imageio\\plugins\\pyav.py:886\u001b[39m, in \u001b[36mPyAVPlugin.init_video_stream\u001b[39m\u001b[34m(self, codec, fps, pixel_format, max_keyframe_interval, force_keyframes)\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize a new video stream.\u001b[39;00m\n\u001b[32m    851\u001b[39m \n\u001b[32m    852\u001b[39m \u001b[33;03mThis function adds a new video stream to the ImageResource using the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    882\u001b[39m \n\u001b[32m    883\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    885\u001b[39m fps = Fraction.from_float(fps)\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m stream.time_base = Fraction(\u001b[32m1\u001b[39m / fps).limit_denominator(\u001b[38;5;28mint\u001b[39m(\u001b[32m2\u001b[39m**\u001b[32m16\u001b[39m - \u001b[32m1\u001b[39m))\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\av\\container\\output.py:60\u001b[39m, in \u001b[36mav.container.output.OutputContainer.add_stream\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\reisi\\.conda\\envs\\dqn-lunar-lander\\Lib\\site-packages\\av\\codec\\codec.pyx:86\u001b[39m, in \u001b[36mav.codec.codec.Codec.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: expected bytes, NoneType found"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "import numpy as np # Certifique-se de que numpy está importado\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# Assumindo que 'brain' é uma instância da sua classe DQN devidamente inicializada e/ou treinada\n",
        "\n",
        "def show_video_of_model(dqn, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array') # Cria o ambiente com renderização em RGB (imagem por frame)\n",
        "\n",
        "    state, _ = env.reset() # Reinicia o ambiente e recebe o estado inicial\n",
        "    frames = [] # Lista que irá armazenar todos os frames do vídeo\n",
        "    \n",
        "    # Condição para o loop: rodar por um número fixo de passos ou até o episódio terminar e o ambiente resetar\n",
        "    max_steps_per_episode = 1000 # Limite razoável de passos por episódio, ajuste conforme necessário\n",
        "    current_step = 0\n",
        "    \n",
        "    # Loop principal para coletar frames. Roda até o episódio terminar (done) ou o limite de passos/frames ser atingido.\n",
        "    # No caso de LunarLander, um episódio termina quando o lander pousa ou colide.\n",
        "    while True:\n",
        "        frame = env.render() # Captura o frame atual (imagem da simulação)\n",
        "        \n",
        "        # ESSENCIAL: VERIFICA SE O FRAME NÃO É NONE ANTES DE ADICIONAR E CONVERTER\n",
        "        if frame is not None:\n",
        "            frames.append(frame.astype(np.uint8)) # Adiciona o frame à lista, garantindo tipo uint8\n",
        "        \n",
        "        action = dqn.act(state, 0) # Usa o modelo DQN para escolher a ação (exploração = 0)\n",
        "        \n",
        "        # Aplica a ação e avança o ambiente no novo formato do Gymnasium\n",
        "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        \n",
        "        # 'done' é verdadeiro se o episódio terminou por 'terminated' ou por 'truncated' (limite de tempo)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        current_step += 1\n",
        "        \n",
        "        if done or current_step >= max_steps_per_episode: # Verifica se o episódio terminou ou o limite de passos foi atingido\n",
        "            # Se o episódio terminou ou atingiu o limite, podemos sair do loop\n",
        "            # Ou resetar o ambiente se quisermos continuar o vídeo por mais frames (como no seu exemplo anterior com max_frames)\n",
        "            break # Sai do loop para finalizar o vídeo do episódio atual.\n",
        "                  # Se quiser vídeo mais longo, reinicie o ambiente e continue o loop:\n",
        "                  # state, _ = env.reset()\n",
        "                  # current_step = 0 # Reinicia o contador de passos para o novo episódio\n",
        "                  # if len(frames) >= 3000: break # Adicione uma condição para parar de coletar frames totais\n",
        "                  \n",
        "    env.close() # Fecha o ambiente após terminar\n",
        "    imageio.mimsave('video.mp4', frames, fps=30) # Salva os frames como um vídeo .mp4 com 30 quadros por segundo\n",
        "\n",
        "# Chamada da função. Certifique-se que 'brain' é sua instância do agente DQN.\n",
        "# Ex: brain = DQN(state_size, number_actions)\n",
        "# Se você tiver um modelo treinado salvo:\n",
        "# brain.load() \n",
        "show_video_of_model(brain, 'LunarLander-v3')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4') # Procura arquivos .mp4 no diretório\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0] # Seleciona o primeiro vídeo encontrado\n",
        "        video = io.open(mp4, 'rb').read() # Lê o vídeo em modo binário\n",
        "        encoded_video = base64.b64encode(video) # Codifica o vídeo em base64\n",
        "        display(HTML(f'''\n",
        "                <video width=\"640\" height=\"480\" controls>\n",
        "                    <source src=\"data:video/mp4;base64,{encoded_video.decode('ascii')}\" type=\"video/mp4\">\n",
        "                </video>\n",
        "                ''')) # Mostra o vídeo embutido em HTML\n",
        "    else:\n",
        "        print(\"ERROR: Video not found\") # Exibe mensagem de erro se nenhum vídeo for encontrado\n",
        "\n",
        "show_video() # Executa a função para exibir o vídeo"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dqn-lunar-lander",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
