{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IRGarrido/Lunar-Lander-DQN/blob/master/Lunar_Lander_DQN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJHu3NYuUJn9"
      },
      "source": [
        "\n",
        "## Lunar Lander V3 - Deep Q-Learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CnaDqSOUSDK"
      },
      "source": [
        "**Dependências:**\n",
        "```\n",
        " pip install swig\n",
        " pip install gymnasium[box2d]\n",
        "```\n",
        "\n",
        "*   OS\n",
        "*   Random\n",
        "*   Numpy\n",
        "*   Torch\n",
        "*   Gymnasium\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação de Dependências"
      ],
      "metadata": {
        "id": "0H1Qf9BA2LZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE-jUdEpyAp8",
        "outputId": "12726400-0090-47fd-a02d-81690ad5e8cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EtOtA-a5TT2q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn # P/ Rede Neural\n",
        "import torch.optim as optim # Otimizador\n",
        "import torch.nn.functional as F # Funções de ativação, perda, convolução, etc (ReLU)\n",
        "import torch.autograd as autograd # Diferenciação automática (gradiente)\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Geração de Ambiente Gymnasium"
      ],
      "metadata": {
        "id": "kIVp2qKI2TiP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zowvpGAfTw8P",
        "outputId": "ac7ce23e-2a20-4bc6-d4bf-a06ac21f0869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (8,)\n",
            "State size:  8\n",
            "Number of actions:  4\n",
            "Observation space:  Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n # 0 - Do Nothing, 1 - Fire left, 2 - Fire down, 3 - Fire right\n",
        "observation_space = env.observation_space\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)\n",
        "print('Observation space: ', observation_space)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrutura de Memória"
      ],
      "metadata": {
        "id": "lgr9LQfF2rEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memória de aprendizado\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, event):\n",
        "        self.memory.append(event)\n",
        "        # Deleta a memória mais antiga se estiver cheio (capacidade).\n",
        "        if len(self.memory) > self.capacity:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Amostragem aleatória do tamanho do batch size\n",
        "        experiences = random.sample(self.memory, k = batch_size)\n",
        "        # Converte em tensores pytorch para serem processados pela rede neural\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "        return (states, actions, rewards, next_states, dones)"
      ],
      "metadata": {
        "id": "xe4jl_Rn180Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição da Rede Neural"
      ],
      "metadata": {
        "id": "9vx4vcCV2h-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* taxa de aprendizado: 0.0005\n",
        "* tamanho de batela: 100\n",
        "* coeficiente gamma: 0.99\n",
        "* tamanho da memória: 100 mil amostras\n",
        "* taxa de atualização (parâmetro tau): 0.001\n",
        "* Número de camadas: 3 (8 - 64 - 4)\n",
        "* Função de ativação: ReLU - ReLU"
      ],
      "metadata": {
        "id": "XQnIbzwA2v0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TaCPvqVJTyVH"
      },
      "outputs": [],
      "source": [
        "# Parâmetros de aprendizado\n",
        "learning_rate = 5e-4\n",
        "batch_size =100 #Tamanho da batela\n",
        "discount_factor = 0.99 # gamma\n",
        "replay_buffer_size = int(1e5) #Memory\n",
        "interpolation_parameter = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rede Neural\n",
        "class Network(nn.Module):\n",
        "  def __init__(self,state_size,number_actions,seed=42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64) # 3 Camadas de neurônios totalmente conectadas\n",
        "    self.fc3 = nn.Linear(64, number_actions)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    q_values = self.fc3(x)\n",
        "    return q_values"
      ],
      "metadata": {
        "id": "8iUtccNoXKEw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(): # Implementando o aprendizado DQN adaptado\n",
        "  def __init__(self,state_size,number_actions):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size =state_size\n",
        "    self.action_size = number_actions\n",
        "    self.memory = ReplayMemory(replay_buffer_size) # Memória\n",
        "    self.model = Network(state_size,number_actions).to(self.device) # Rede Neural\n",
        "    self.optmize = optim.Adam(self.model.parameters()) # Otimizador\n",
        "    self.target_network = Network(state_size,number_actions).to(self.device)\n",
        "    self.step=0\n",
        "\n",
        "  def step(self,state,action,reward,next_state,done):\n",
        "    self.memory.push((state,action,reward,next_state,done)) # Armazena no buffer\n",
        "    self.t_step = (self.t_step +1 )%4 # Atualiza os passos\n",
        "    if self.t_step == 0: # Verifica se é hora de treinar\n",
        "      if len(self.memory.memory) > batch_size:\n",
        "        experiences = self.memory.sample(batch_size)\n",
        "        self.learn(experiences,discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) # Tensor\n",
        "    self.model.eval() # Modo de avaliação\n",
        "    with torch.no_grad():\n",
        "      action_values = self.model(state) # Cálculo de Q-values\n",
        "    self.model.train() # Treinamento\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size)) # Escolhe uma ação com maior Q-value\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences # Extrai as memorias\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1) # Calcula Q-values alvo\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones) # Equação de Bellman (Diferença Temporal)\n",
        "    q_expected = self.model(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets) # Calcula perdas\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.model, self.target_qnetwork, interpolation_parameter) # Propaga e atualiza a rede\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n"
      ],
      "metadata": {
        "id": "DthgH9iF_pbk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendizado por Reforço (Deep Q-Learning)"
      ],
      "metadata": {
        "id": "aas5kGhw6dXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brain = DQN(state_size,number_actions)\n",
        "reward = 0"
      ],
      "metadata": {
        "id": "o2CkCMQHxsWc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.reset())"
      ],
      "metadata": {
        "id": "HyBvfYTu_D5m",
        "outputId": "8bcd8cc9-618d-4b99-9670-89e3303421ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.20162964e-04  9.38680655e-01  1.21504024e-02 -1.28852963e-01\n",
            " -1.32402871e-04 -2.75225546e-03  0.00000000e+00  0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = brain.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    brain.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(brain.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "id": "RQEX79fP-HG_",
        "outputId": "b6058de7-b229-402e-ce27-c99d2af77272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-49-3308895525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_number_timesteps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-36-2553881733.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mm_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;31m# Main engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfuWJlI7-74p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}